{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Building String Scoring Models ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Reference:__ Some code snippets for the initial part (pre-processing etc.) are taken from the blog mentioned in the Project description file. [https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py](https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this blog is to build string scoring models which will then be combined into a single language detector.\n",
    "\n",
    "The scoring models are recurrent neural networks (RNNs), using ***Long-Short Term Memory (LSTM)*** units. LSTMs are particularly well suited for learning patterns in long input sequences (Ex: text, speech), without being affected by the vanishing gradient problem that plagues deep neural networks. **Given a sequence of input characters**, these neural networks are trained to **predict the next character** in that sequence.\n",
    "\n",
    "These models are then used to calculate the **likelihood** that the language of a test sequence is the same as the language on which the model is trained. For a **binary classification** problem, the **difference of likelihoods** is used as the decision value, whereas the **maximum likelihood** is used as the decision function for **multi-language models**.\n",
    "\n",
    "For the extra credit part of this assignment, we have attempted a few experiments, including improving the binary models' accuracy and implementing a multi-language classification model. These experiments and their results are documented and presented in separate notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries and Modules ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Activation, Dense, Dropout, LSTM\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.optimizers import RMSprop\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Model Parameters ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sys.setrecursionlimit(10000)     # setting this higher in case a saved model is loaded\n",
    "RANDOM_STATE = 100\n",
    "np.random.seed(100)              # set seed for reproducibility\n",
    "tf.random.set_seed(100)          # set seed for reproducibility\n",
    "\n",
    "val_frac = 0.2                   # fraction of data to be set as validation set\n",
    "test_frac = 0.2                  # fraction of data to be set as test set\n",
    "\n",
    "LANGUAGES = ['eng', 'frn']       # list of languages for classification\n",
    "\n",
    "maxlen = 40                      # length of sliding window (input sequence) to the LSTM model\n",
    "step = 1                         # the step size for cutting the input sequence into redundant sequences of size maxlen\n",
    "\n",
    "len_test = 5                     # length of the test substrings\n",
    "num_test = 100                   # number of test substrings taken from ONE particular language\n",
    "\n",
    "EPOCHS = 5                       # number of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define methods to read and featurize the text ###\n",
    "The text is split into overlapping sentences of fixed length using a sliding window. A charset containing all the unique characters across all languages is used for each language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(lang):\n",
    "    \n",
    "    '''\n",
    "    This function takes a language name as input, reads the particular contents of the language file from the folder 'subset',\n",
    "    converts the text to lower case and makes a list of the unique characters encountered. The text, as a string, and the unique\n",
    "    characters, as a list, are returned by the function.\n",
    "    '''\n",
    "    \n",
    "    # path where the language text files are stored\n",
    "    path = './subset/{}.txt'.format(lang)\n",
    "\n",
    "    text = open(path).read().lower()\n",
    "    print('\\n\\n{} corpus length: {}'.format(lang, len(text)))\n",
    "    \n",
    "    # store the unique characters as a list\n",
    "    chars = sorted(list(set(text)))\n",
    "    print('unique chars:', len(chars))\n",
    "    \n",
    "    return text, chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_sequences(text):\n",
    "    '''\n",
    "    This function recieves a string as input, and cuts it intoe semi-redundant sequences of maxlen characters each. Basically\n",
    "    sliding a window of length 'maxlen' over the text, and taking increments of 'step' at a time. These sequences are stored in\n",
    "    a list called 'senteces'. The last character of each sequence is stored in a list called 'next_chars'. The function returns\n",
    "    the two lists, 'sentences' and 'next_chars'.\n",
    "    '''\n",
    "    sentences = []\n",
    "    next_chars = []\n",
    "    \n",
    "    for i in range(0, len(text) - maxlen, step):\n",
    "        sentences.append(text[i: i + maxlen])\n",
    "        next_chars.append(text[i + maxlen])\n",
    "    \n",
    "    print('nb sequences:', len(sentences))\n",
    "    \n",
    "    return sentences, next_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(charset, sentences, next_chars):\n",
    "    '''\n",
    "    This function takes as input a characterset (list of unique characters), sentences (list of redundant character sequences) \n",
    "    and next_chars (list of the last character of each sequence in senetences). Then it generates the vector reprsentation of\n",
    "    sentences and next_chars, X and y respectively.\n",
    "    '''\n",
    "    \n",
    "    print('Vectorizing text...')\n",
    "    \n",
    "    # X: 3 dimensional, dimension 1: number of sequences (number of inputs), dimension 2: the length of the sliding\n",
    "    # window, maxlen, dimension 3: length of the characterset. So for a given sequence or input, dimension 2 represents \n",
    "    # the different characters in the sequence and for each character only one value in the charset is set to 1 (1-hot encoding)\n",
    "    \n",
    "    X = np.zeros((len(sentences), maxlen, len(charset)), dtype=np.bool)\n",
    "    \n",
    "    \n",
    "    # Y: 2 dimensional, dimension 1: number of inputs (each input is a single chracter which is the last character in\n",
    "    # the corresponding sequence contained in X). The second dimension is the character set representing the 1-hot encoding\n",
    "    # scheme for the character\n",
    "    \n",
    "    y = np.zeros((len(sentences), len(charset)), dtype=np.bool)\n",
    "    \n",
    "    # convert the characters to dictinary for easy access and for 1-hot encoding\n",
    "    char_indices = dict((c, i) for i, c in enumerate(charset))\n",
    "    \n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for t, char in enumerate(sentence):\n",
    "            X[i, t, char_indices[char]] = 1\n",
    "        y[i, char_indices[next_chars[i]]] = 1\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define basic methods for building the models\n",
    "The data is randomly split into a train set (80%) and a holdout test set (20%). The model is trained only on the train set, i.e. it never sees any samples from the holdout set during training.  \n",
    "\n",
    "The neural network models for each language are saved after training, so that they can be loaded later for reuse without retraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_n_save(X, y, charset, lang):\n",
    "    '''\n",
    "    Takes input matrix X, labels y, the characterset and language name. Splits X, y into the training and test sets, stores\n",
    "    them in a folder called 'splits' and returns the splits as well.\n",
    "    '''\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=test_frac, random_state=RANDOM_STATE)\n",
    "    \n",
    "    np.savez_compressed('./splits/{}.npz'.format(lang),\n",
    "                        xtrain=X_train,\n",
    "                        xtest=X_test,\n",
    "                        ytrain=y_train,\n",
    "                        ytest=y_test,\n",
    "                       charset=charset)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(lang, charset):\n",
    "    '''\n",
    "    Builds and returns a single layered LSTM model, with size 128. The input shape is a tuple (maxlen, lenght of the \n",
    "    characterset). maxlen is the string sequence lenght and the charset will contain the probabilities of each character \n",
    "    in the sequence. The activation used for the output is 'softmax' to output normalized probability values for each character \n",
    "    in the set.\n",
    "    '''\n",
    "    \n",
    "    model = Sequential(name=lang)\n",
    "    \n",
    "    # add an LSTM layer\n",
    "    model.add(LSTM(128, input_shape=(maxlen, len(charset))))\n",
    "    \n",
    "    # add a dense layer with size as the length of the characterset\n",
    "    model.add(Dense(len(charset)))\n",
    "    \n",
    "    # softmax activation\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    optimizer = RMSprop(learning_rate=0.01)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_preds(preds, temperature=1.0):\n",
    "    '''\n",
    "    This function takes predictions and normalizes them so that they sum to 1. In this notebook, this function is actually\n",
    "    redundant as the predictions are already normalized values.\n",
    "    '''\n",
    "    \n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_n_save(model, X, y):\n",
    "    '''\n",
    "    Trains and saves a model on the input X and output y. The number of epochs and fraction of data used for validation are\n",
    "    taken from the model parameters set in the initial code blocks at the start of the notebook\n",
    "    '''\n",
    "\n",
    "    model.fit(X, y, batch_size=128, epochs=EPOCHS, verbose=2, validation_split=val_frac)\n",
    "    \n",
    "    model.save('./models/{}.h5'.format(model.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the neural networks\n",
    "Using the methods defined above, we now read the data, split it and train the neural network models using the train split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# read the language files and store all unique characters in a common characterset\n",
    "charset = set()\n",
    "for lang in LANGUAGES:\n",
    "    text, chars = read_file(lang)\n",
    "    charset = charset.union(chars)\n",
    "\n",
    "# for each language\n",
    "for lang in LANGUAGES:\n",
    "    \n",
    "    # get the text of the language\n",
    "    text, _ = read_file(lang)\n",
    "    \n",
    "    # cut it into semi redundant character sequences, and also get the last character of each sequence\n",
    "    sentences, next_chars = cut_sequences(text)\n",
    "    \n",
    "    # vectorize sentences and next_chars\n",
    "    X, y = vectorize(charset, sentences, next_chars)\n",
    "    \n",
    "    # get the training and test splits\n",
    "    X_train, X_test, y_train, y_test = split_n_save(X, y, charset, lang)\n",
    "    \n",
    "    # train and save the language model\n",
    "    model = build_model(lang, charset)\n",
    "    train_n_save(model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the models, and calculate log likelihoods for the test strings\n",
    "100 substrings of length 5 are randomly sampled from the holdout sets of each language, and these strings are used as the test sequences for the neural network models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_the_model(lang):\n",
    "    '''\n",
    "    Given a language name, load the model and its associated data and return them\n",
    "    '''\n",
    "    \n",
    "    model = load_model('./models/{}.h5'.format(lang))\n",
    "    \n",
    "    data = np.load('./splits/{}.npz'.format(lang))\n",
    "    \n",
    "    X_train = data['xtrain']\n",
    "    X_test = data['xtest']\n",
    "    y_train = data['ytrain']\n",
    "    y_test = data['ytest']\n",
    "    \n",
    "    return model, X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def select_subs(X_test): \n",
    "    '''\n",
    "    Given the test sequence, randomly select and return num_test(100) number of len_test(5) character substrings\n",
    "    '''\n",
    "    test = np.zeros((num_test, len_test, len(charset)), dtype=np.bool)\n",
    "    \n",
    "    # randomly select num_test sequences\n",
    "    lines = np.random.choice(len(X_test), num_test, replace = False)\n",
    "    \n",
    "    # from each randomly selected sequence, randomly select a continuous sequence of len_test substrings\n",
    "    for counter, line in enumerate(lines):\n",
    "        \n",
    "        # random substring selection\n",
    "        i = np.random.choice(X_test.shape[1] - len_test)\n",
    "        test[counter,:,:] = X_test[line,i:i+len_test,:]\n",
    "        \n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create the test substring dataset and the the true labels\n",
    "\n",
    "test = np.empty((0, len_test, len(charset)), dtype = np.bool)\n",
    "\n",
    "# one hot encoding for the labels\n",
    "labels_test = np.zeros((num_test*len(LANGUAGES), len(LANGUAGES)), dtype = np.int64)    \n",
    "\n",
    "for i, lang in enumerate(LANGUAGES):\n",
    "    data = np.load('./splits/{}.npz'.format(lang))\n",
    "    test = np.vstack((test,select_subs(data['xtest'])))\n",
    "    labels_test[(i*num_test):(i+1)*num_test, i] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def load_all_models():\n",
    "    '''\n",
    "    Load all models and return as a list\n",
    "    '''\n",
    "    \n",
    "    models = []\n",
    "    for count, lang in enumerate(LANGUAGES, start = 1):\n",
    "        print(\"loading model no: {}\".format(count))\n",
    "        models.append(load_the_model(lang)[0])\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the predictions\n",
    "Initially, each model is seeded with an unbiased start sequence (a sequence in which every position is equally likely to be any character from the entire charset). The probability of the first letters of the test sequences are then found.\n",
    "\n",
    "We go on to predict the probability of the next characters of the test sequences given the previous characters, one at a time.\n",
    "\n",
    "The final predicted value (y_hat) is the difference of log probabilities of the english and french models.\n",
    "$$\n",
    "y_{hat} = log[Pr(string | eng)] - log[Pr(string | frn)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def predict(test, model):\n",
    "    '''\n",
    "    Function takes a test set containing substrings of fixed length in different languages and a language specific model. The\n",
    "    model then predicts the likelihood of each substring belonging to that specific language. \n",
    "    '''\n",
    "    \n",
    "    num_samples = test.shape[0]\n",
    "\n",
    "    likelihoods = np.zeros(num_samples, dtype=np.float64)\n",
    "\n",
    "    start_shape = (num_samples, maxlen, len(charset))\n",
    "\n",
    "    # here we set the probabilities of all characters in start sequence to be equal\n",
    "    # essentialy for the first character to be predicted the model has no context\n",
    "    start_value = 1.0 / len(charset)\n",
    "    start = np.full(start_shape, start_value, dtype=np.float64)\n",
    "\n",
    "    test_sequences = start\n",
    "\n",
    "    # enumerate over characters of all test substrings and predict their likelihoods\n",
    "    for i, chars in enumerate(zip(*test)):\n",
    "        preds = model.predict(test_sequences, verbose=0)\n",
    "        preds = normalize_preds(preds)\n",
    "\n",
    "        # calculate the log probability\n",
    "        likelihoods += np.log(preds[test[:, i, :]])\n",
    "\n",
    "        # move the sliding input sequence by appending the test character from the substring\n",
    "        next_chars = np.reshape(chars, (test_sequences.shape[0], 1, test_sequences.shape[2]))\n",
    "        test_sequences = np.concatenate((test_sequences[:, 1:, :], next_chars), axis=1)\n",
    "\n",
    "    return likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Reciever Operating Characteristics (ROC) curve\n",
    "The ROC curve shows the true positive rate vs the false positive rate for a binary classification task. Both the neural network models are fed with the same test sequences, and the predicted likelihoods are used to plot the ROC curve using matplotlib and sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def accuracy_and_roc():\n",
    "    '''\n",
    "    Calcluate the accuracy and plot the ROC\n",
    "    '''\n",
    "    \n",
    "    # get all models as a list\n",
    "    models = load_all_models()\n",
    "    \n",
    "    # get the likelihood for each model\n",
    "    likelihoods = np.zeros((len(test), len(models)), dtype = np.float64)\n",
    "    for i, model in enumerate(models):\n",
    "        likelihoods[:,i] = predict(test, model)\n",
    "    \n",
    "    \n",
    "    # calculate the accuracy\n",
    "    count = 0\n",
    "    for i in range(len(test)):\n",
    "        if( (likelihoods[i,0] > likelihoods[i,1]) == bool(labels_test[i,0]) ): # correct prediction\n",
    "            count += 1\n",
    "    accuracy = (count/len(labels_test))*100.0\n",
    "    \n",
    "    # plot the ROC on semilogx\n",
    "    fpr, tpr, thresholds = roc_curve(labels_test[:,0], likelihoods[:,0] - likelihoods[:,1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    \n",
    "    print (\"\\nAccuracy: {:0.3f}, Area under the curve: {:0.3f}\".format(accuracy, roc_auc))\n",
    "    \n",
    "    plt.plot(fpr, tpr, lw=2, color='blue', linestyle = 'solid', label='area={:0.3f}'.format(roc_auc))\n",
    "    plt.plot([0, 1], [0, 1], linestyle='dashed', lw=2, color='k', label='Luck')\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristics')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()  \n",
    "    \n",
    "    return (accuracy, roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Calculate and print the accuracy, ROC and area under the curve\n",
    "accuracy, area = accuracy_and_roc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Questions ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. Is this model good ? ###\n",
    "  \n",
    "_Ans:_  Given the small amount of training data and the small size of the test substring the accuracy of the model and the area under the curve seem pretty good. Intutively, if the length of the substring is increased from 5, the accuracy should increase since more language specific semantics, structural and contextual information will be captured in a longer substring which will help in classfication. As a test, when we increased the length of the substring to 20 from 5, the accuracy and the area under the curve both shot up to 96% and 0.97 respectively reaching almost perfect classfication\n",
    "\n",
    "\n",
    "### Q2. What are at least three alternatives to language detection that you can think of or find on the internet? What are the pros and cons of each approach? ###\n",
    "\n",
    "_Ans: Three alternatives are:_\n",
    "  \n",
    "  a) Language detection using N-grams: Based on creating a language profile of N-gram frequencies (N-gram matrix) using the training document. The test text is then converted to an N-gram profile using similar technique and the distance between the test text and all language profiles is calculated. Whichever distance is the least is the most probable language.\n",
    "  \n",
    "  Pros:\n",
    "  \n",
    "  * Simple and easy to understand, not very computationally heavy (at least as compared to LSTM)\n",
    "  \n",
    "  Cons:\n",
    "  \n",
    "  * Since its based on language N-gram profiles, it requires a lot of preprocessing and is unable to use contextual informaiton in the language. For example, whereas LSTMS can learn complex language structures, like quotation marks etc, the N-gram model cannot\n",
    "  \n",
    "  b) Bag of Words (_or frequent words counting_): Make an extensive collection of words (dictionary) for each language. Then, given a text, classify based on the frequency of words in the text.\n",
    " \n",
    "  Pros:\n",
    "  \n",
    "  * Relatively simple with few hyperparameters. For example, N-grams has a number of parameters, like, the length of the N-gram sequence and the overlap in the sequences.\n",
    "  \n",
    "  Cons:\n",
    "  \n",
    "  * If a word in the test set was not in the training set, then bag of words has no interpretation for that word. On the other hand, an N-gram scheme can still have some insight about a word based on the N-gram sequence in that word\n",
    "  \n",
    "  * It needs large storage space to store an entire language dictionary of all possible words and their forms (eg. fast, faster, fastest, etc.)\n",
    "  \n",
    "  * Susceptible to spelling errors.\n",
    "  \n",
    "  c) Markov Models for Language Identification: The occurrences of letters in a word is regarded as a stochastic process and the word is represented as a Markov chain where letters are states.\n",
    " \n",
    "  Pros:\n",
    "  \n",
    "  * Compared to the N-grams method, Markov chain-based system is much faster for identification since the number of letters is smaller than the number of N-grams (say tri-grams) in any language.\n",
    "  \n",
    "  Cons:\n",
    "  \n",
    "  * Language structure is not considered\n",
    "  \n",
    "  d) Small words technique: Similar to N-grams (tri-grams) technique, the difference being that instead of high frequency tri-grams (which can be high in number), only frequently occuring common short words (like _an, the , and_ etc.) are considered.\n",
    " \n",
    "  Pros:\n",
    "  \n",
    "  * Compared to the tri-grams method, the small words technique is faster since for a given text input, there are less words than tr-grams and hence the sentence probability calculation is faster.\n",
    "  \n",
    "  Cons:\n",
    "  \n",
    "  * Accuracy is low for short texts and sentences. This is because in shorter senteces, there is a greater chance that no language characteristic words are used.\n",
    "  \n",
    "\n",
    "### Q3. Describe 5 ways to improve the model ###\n",
    "\n",
    "_Ans: 5 ways to improve the model are:_\n",
    "  \n",
    "  1) __Hyperparamter Tuning:__ There are a number of hyperparameters in this model, (number of LSTM layers, size of layers, dropout, length of input sequence, step size etc.) which can be optimized to give better results ---> Advantage: Improvement in classfication accuracy at the cost of increased computation cost during hyperparameter optimization.\n",
    "  \n",
    "  2) __Early Stopping:__ Optmize the number of epochs by setting validation loss decrease criterion. For example, monitor validation loss and if the validation loss decreases by less than a set threshold (say 1%), then stop training. ---> Advantage: More efficient training in terms of time required, i.e, one does not always have to run a fixed number of epochs. Also, this method avoids overfitting by interrupting model fitting when validation loss starts to increase.\n",
    "  \n",
    "  3) __Diverse Inputs:__ Feeding the model, diverse styles and topics from each language under consideration should increase accuracy. The more diverse the input, like poetry, scientific texts etc. and more the type of topics covered, like sports, technology etc. the more language specific content the model will learn and will be able to generalize well.This will, though, increase computation cost at training time because of the increased training size.\n",
    "  \n",
    "  4) __Increase Input Sequence Length:__ The longer the input sequence, the more language specific structural elements the model can learn. For example, if the input is source code, and if the input sequence is long enough, then the model can even learn the syntax, for example, when and what type of braces to open and close etc. Obvipously this will come at the cost of larger computation complexity.\n",
    "  \n",
    "  5) __Parallel Processing:__ Use of GPUs, for example, will decrease the time required to train the models and the increased computational power can allow the modeler to explore more in-depth hyperparameter tuning ultimately leading to a more accurate model."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
